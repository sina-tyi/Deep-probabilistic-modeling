{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Semi-Supervised VAE\n",
    "\n",
    "The semi-supervised setting represents an interesting intermediate case where some of the data is labeled and some is not. It is also of great practical importance, since we often have very little labeled data and much more unlabeled data. We’d clearly like to leverage labeled data to improve our models of the unlabeled data.\n",
    "\n",
    "The semi-supervised setting is also well suited to generative models, where missing data can be accounted for quite naturally—at least conceptually. As we will see, in restricting our attention to semi-supervised generative models, there will be no shortage of different model variants and possible inference strategies. Although we’ll only be able to explore a few of these variants in detail, hopefully you will come away from the tutorial with a greater appreciation for the abstractions and modularity offered by probabilistic programming.\n",
    "\n",
    "So let’s go about building a generative model. We have a dataset $D$ with $N$ datapoints,\n",
    "\n",
    "$ D = $ {(xi, yi)}$  $\n",
    "\n",
    "where the  {xi} are always observed and the labels {yi} are only observed for some subset of the data. Since we want to be able to model complex variations in the data, we’re going to make this a latent variable model with a local latent variable {zi} private to each pair (xi, yi). Even with this set of choices, a number of model variants are possible: we’re going to focus on the model variant depicted in Figure 1.\n",
    "\n",
    "<img src=\"assets/ss_vae_m2.png\"  width=\"180\" height=\"200\">\n",
    "\n",
    "For convenience—and since we’re going to model MNIST in our experiments below—let’s suppose the {xi} are images and the {yi} are digit labels. In this model setup, the latent random variable {zi} and the (partially observed) digit label jointly generate the observed image. The {zi} represents everything but the digit label, possibly handwriting style or position. Let’s sidestep asking when we expect this particular factorization of (xi, yi, zi) to be appropriate, since the answer to that question will depend in large part on the dataset in question (among other things). Let’s instead highlight some of the ways that inference in this model will be challenging as well as some of the solutions that we’ll be exploring in the rest of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge of Inference\n",
    "\n",
    "For concreteness we’re going to continue to assume that the partially-observed {yi} are discrete labels; we will also assume that the {zi} are continuous.\n",
    "\n",
    "- If we apply the general recipe for stochastic variational inference to our model (see SVI Part I) we would be sampling the discrete (and thus non-reparameterizable) variable {yi} whenever it’s unobserved. As discussed in SVI Part III this will generally lead to high-variance gradient estimates.\n",
    "\n",
    "- A common way to ameliorate this problem—and one that we’ll explore below—is to forego sampling and instead sum out all ten values of the class label {yi} when we calculate the ELBO for an unlabeled datapoint {xi} . This is more expensive per step, but can help us reduce the variance of our gradient estimator and thereby take fewer steps.\n",
    "\n",
    "- Recall that the role of the guide is to ‘fill in’ latent random variables. Concretely, one component of our guide will be a digit classifier $q\\phi(y|x)$ that will randomly ‘fill in’ labels {yi} given an image {xi}. Crucially, this means that the only term in the ELBO that will depend on $q\\phi(.|x)$ is the term that involves a sum over unlabeled datapoints. This means that our classifier $q\\phi(.|x)$—which in many cases will be the primary object of interest—will not be learning from the labeled datapoints (at least not directly).\n",
    "\n",
    "- This seems like a potential problem. Luckily, various fixes are possible. Below we’ll follow the approach in reference, which involves introducing an additional objective function for the classifier to ensure that the classifier learns directly from the labeled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sina/Anaconda/anaconda3/envs/pyroenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pyro.contrib.examples.util import MNIST\n",
    "import torch.nn as nnp\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('1.8.4')\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(self, xs, ys=None):\n",
    "    # register this pytorch module and all of its sub-modules with pyro\n",
    "    pyro.module(\"ss_vae\", self)\n",
    "    batch_size = xs.size(0)\n",
    "\n",
    "    # inform Pyro that variables in the batch of xs, ys are conditionally independent\n",
    "    with pyro.plate(\"data\"):\n",
    "        # sample the handwritting style from the constant prior distribution\n",
    "        prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "        prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "        zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "        # if the label y (which digit to write) is supervised, sample from the\n",
    "        # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "        alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "        ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "\n",
    "        # finally, score the image (x) using the handwritting style (z) and\n",
    "        # the class label y (which digit to write) against the\n",
    "        # parametrized distribution p(x|y, z) = bernouli(decoder(y,z))\n",
    "        # where 'decoder' is neural network\n",
    "        loc = self.decoder([zs, ys])\n",
    "        pyro.sample(\"x\", dist.Bernouli(loc).to_event(1), obs=xs)\n",
    "\n",
    "def guide(self, xs, ys=None):\n",
    "    with pyro.plate(\"data\"):\n",
    "        # if the class label (the digit) is not supervised, sample\n",
    "        # (and score) the digit with the variational distribution\n",
    "        # q(y|x) = categorical(alpha(x))\n",
    "        if ys is None:\n",
    "            alpha = self.encoder_y(xs)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
    "\n",
    "        # sample (and score) the latent handwritting-style with the variational\n",
    "        # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "        loc, scale = self.encoder_z([xs, ys])\n",
    "        pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definitions\n",
    "In our experiments we use the same network configurations as used in reference [1]. The encoder and decoder networks have one hidden layer with \n",
    " hidden units and softplus activation functions. We use softmax as the activation function for the output of encoder_y, sigmoid as the output activation function for decoder and exponentiation for the scale part of the output of encoder_z. The latent dimension is 50.\n",
    "\n",
    "## MNIST Pre-Processing\n",
    "We normalize the pixel values to the range [0.0, 1.0]. We use the MNIST data loader from the torchvision library. The testing set consists of 10000 examples. The default training set consists of 600000 examples. We use the first 50000 examples for training (divided into supervised and un-supervised parts) and the remaining 10000 images for validation. For our experiments, we use 4 configurations of supervision in the training set, i.e. we consider 3000, 1000, 600, and 100 supervised examples selected randomly (while ensuring that each class is balanced).\n",
    "\n",
    "## The Objective Function\n",
    "The objective function for this model has the two terms:\n",
    "\n",
    "  <img src=\"assets/1.png\"  width=\"350\" height=\"80\">\n",
    " \n",
    "To implement this in Pyro, we setup a single instance of the SVI class. The two different terms in the objective functions will emerge automatically depending on whether we pass the step method labeled or unlabeled data. We will alternate taking steps with labeled and unlabeled mini-batches, with the number of steps taken for each type of mini-batch depending on the total fraction of data that is labeled. For example, if we have 1,000 labeled images and 49,000 unlabeled ones, then we’ll take 49 steps with unlabeled mini-batches for each labeled mini-batch. (Note that there are different ways we could do this, but for simplicity we only consider this variant.) The code for this setup is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "#setup the optimzer\n",
    "adam_params = {\"lr\": 0.0003}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "#setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Summing Out Discrete Latents\n",
    "\n",
    "As highlighted in the introduction, when the discrete latent labels $y$ are not observed, the ELBO gradient estimates rely on sampling from $q(y|x)$. These gradient estimates can be very high-variance, especially early in the learning process when the guessed labels are often incorrect. A common approach to reduce variance in this case is to sum out discrete latent variables, replacing the Monte Carlo expectation\n",
    "\n",
    "  <img src=\"assets/2.png\"  width=\"350\" height=\"80\">\n",
    "\n",
    "with an explicit sum\n",
    "\n",
    "  <img src=\"assets/3.png\"  width=\"350\" height=\"80\">\n",
    "\n",
    "This sum is usually implemented by hand, as in [1], but Pyro can automate this in many cases. To automatically sum out all discrete latent variables (here only $y$), we simply wrap the guide in config_enumerate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(model, config_enumerate(guide), optimizer, loss = TraceEnum_ELBO(max_plate_nesting=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mode of operation, each svi.step(...) computes a gradient term for each of the ten latent states of $y$. Although each step is thus 10x more expensive, we’ll see that the lower-variance gradient estimate outweighs the additional cost.\n",
    "\n",
    "Going beyond the particular model in this tutorial, Pyro supports summing over arbitrarily many discrete latent variables. Beware that the cost of summing is exponential in the number of discrete variables, but is cheap(er) if multiple independent discrete variables are packed into a single tensor (as in this tutorial, where the discrete labels for the entire mini-batch are packed into the single tensor $y$). To use this parallel form of config_enumerate(), we must inform Pyro that the items in a minibatch are indeed independent by wrapping our vectorized code in a with pyro.plate(\"name\") block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Variant: Standard Objective Function, Better Estimator\n",
    "\n",
    "Now that we have the tools to sum out discrete latents, we can see if doing so helps our performance. First, as we can see from Figure 3, the test and validation accuracies now evolve much more smoothly over the course of training. More importantly, this single modification improved test accuracy from around 20% to about 90% for the case of 3000 labeled examples. See Table 1 for the full results. This is great, but can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Variant: Adding a Term to the Objective\n",
    "\n",
    "For the two variants we’ve explored so far, the classifier $q(y|x)$ doesn’t learn directly from labeled data. As we discussed in the introduction, this seems like a potential problem. One approach to addressing this problem is to add an extra term to the objective so that the classifier learns directly from labeled data. The modified objective function is given by:\n",
    "\n",
    "  <img src=\"assets/4.png\"  width=\"350\" height=\"80\">\n",
    "\n",
    "To learn using this modified objective in Pyro we do the following:\n",
    "\n",
    "- We use a new model and guide pair (see the code snippet below) that corresponds to scoring the observed label $y$ for a given image $x$ against the predictive distribution $q(y|x)$\n",
    "\n",
    "- We specify the scaling factor $a'$ (aux_loss_multiplier in the code) in the pyro.sample call by making use of poutine.scale. Note that poutine.scale was used to similar effect in the Deep Markov Model to implement KL annealing.\n",
    "\n",
    "- We create a new SVI object and use it to take gradient steps on the new objective term\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_classify(self, xs, ys=None):\n",
    "    pyro.module(\"ss_vae\", self)\n",
    "    with pyro.plate(\"data\"):\n",
    "        #this here is the extra term to yield an auxiliary loss\n",
    "        #that we do gradient descent on\n",
    "        if ys is not None:\n",
    "            alpha = self.encoder_y(xs)\n",
    "            with pyro.poutine.scale(scale=self.aux_loss_multiplier):\n",
    "                pyro.sample(\"y_aux\", dist.OneHotCategorical(alpha), obs=ys)\n",
    "\n",
    "def guide_classify(xs, ys):\n",
    "    # the guide is trivial, since there are no latent random variables\n",
    "    pass\n",
    "\n",
    "svi_aux = SVI(model_classify, guide_classify, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run inference in Pyro with the additional term in the objective, we outperform both previous inference setups. For example, the test accuracy for the case with 3000 labeled examples improves from 90% to 96% (see Figure 4 below and Table 1 in the next section). Note that we used validation accuracy to select the hyperparameter $a'$.\n",
    "\n",
    " <img src=\"assets/5.png\"  width=\"350\" height=\"250\">\n",
    " <img src=\"assets/6.png\"  width=\"350\" height=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
