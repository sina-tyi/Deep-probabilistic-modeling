{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Semi-Supervised VAE\n",
    "\n",
    "The semi-supervised setting represents an interesting intermediate case where some of the data is labeled and some is not. It is also of great practical importance, since we often have very little labeled data and much more unlabeled data. We’d clearly like to leverage labeled data to improve our models of the unlabeled data.\n",
    "\n",
    "The semi-supervised setting is also well suited to generative models, where missing data can be accounted for quite naturally—at least conceptually. As we will see, in restricting our attention to semi-supervised generative models, there will be no shortage of different model variants and possible inference strategies. Although we’ll only be able to explore a few of these variants in detail, hopefully you will come away from the tutorial with a greater appreciation for the abstractions and modularity offered by probabilistic programming.\n",
    "\n",
    "So let’s go about building a generative model. We have a dataset $D$ with $N$ datapoints,\n",
    "\n",
    "$ D = $ {(xi, yi)}$  $\n",
    "\n",
    "where the  {xi} are always observed and the labels {yi} are only observed for some subset of the data. Since we want to be able to model complex variations in the data, we’re going to make this a latent variable model with a local latent variable {zi} private to each pair (xi, yi). Even with this set of choices, a number of model variants are possible: we’re going to focus on the model variant depicted in Figure 1.\n",
    "\n",
    "<img src=\"ss_vae_m2.png\"  width=\"180\" height=\"200\">\n",
    "\n",
    "For convenience—and since we’re going to model MNIST in our experiments below—let’s suppose the {xi} are images and the {yi} are digit labels. In this model setup, the latent random variable {zi} and the (partially observed) digit label jointly generate the observed image. The {zi} represents everything but the digit label, possibly handwriting style or position. Let’s sidestep asking when we expect this particular factorization of (xi, yi, zi) to be appropriate, since the answer to that question will depend in large part on the dataset in question (among other things). Let’s instead highlight some of the ways that inference in this model will be challenging as well as some of the solutions that we’ll be exploring in the rest of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge of Inference\n",
    "\n",
    "For concreteness we’re going to continue to assume that the partially-observed {yi} are discrete labels; we will also assume that the {zi} are continuous.\n",
    "\n",
    "- If we apply the general recipe for stochastic variational inference to our model (see SVI Part I) we would be sampling the discrete (and thus non-reparameterizable) variable {yi} whenever it’s unobserved. As discussed in SVI Part III this will generally lead to high-variance gradient estimates.\n",
    "\n",
    "- A common way to ameliorate this problem—and one that we’ll explore below—is to forego sampling and instead sum out all ten values of the class label {yi} when we calculate the ELBO for an unlabeled datapoint {xi} . This is more expensive per step, but can help us reduce the variance of our gradient estimator and thereby take fewer steps.\n",
    "\n",
    "- Recall that the role of the guide is to ‘fill in’ latent random variables. Concretely, one component of our guide will be a digit classifier $q\\phi(y|x)$ that will randomly ‘fill in’ labels {yi} given an image {xi}. Crucially, this means that the only term in the ELBO that will depend on $q\\phi(.|x)$ is the term that involves a sum over unlabeled datapoints. This means that our classifier $q\\phi(.|x)$—which in many cases will be the primary object of interest—will not be learning from the labeled datapoints (at least not directly).\n",
    "\n",
    "- This seems like a potential problem. Luckily, various fixes are possible. Below we’ll follow the approach in reference, which involves introducing an additional objective function for the classifier to ensure that the classifier learns directly from the labeled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sina/Anaconda/anaconda3/envs/pyroenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pyro.contrib.examples.util import MNIST\n",
    "import torch.nn as nnp\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('1.8.4')\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(self, xs, ys=None):\n",
    "    # register this pytorch module and all of its sub-modules with pyro\n",
    "    pyro.module(\"ss_vae\", self)\n",
    "    batch_size = xs.size(0)\n",
    "\n",
    "    # inform Pyro that variables in the batch of xs, ys are conditionally independent\n",
    "    with pyro.plate(\"data\"):\n",
    "        # sample the handwritting style from the constant prior distribution\n",
    "        prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "        prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "        zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "        # if the label y (which digit to write) is supervised, sample from the\n",
    "        # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "        alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "        ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "\n",
    "        # finally, score the image (x) using the handwritting style (z) and\n",
    "        # the class label y (which digit to write) against the\n",
    "        # parametrized distribution p(x|y, z) = bernouli(decoder(y,z))\n",
    "        # where 'decoder' is neural network\n",
    "        loc = self.decoder([zs, ys])\n",
    "        pyro.sample(\"x\", dist.Bernouli(loc).to_event(1), obs=xs)\n",
    "\n",
    "def guide(self, xs, ys=None):\n",
    "    with pyro.plate(\"data\"):\n",
    "        # if the class label (the digit) is not supervised, sample\n",
    "        # (and score) the digit with the variational distribution\n",
    "        # q(y|x) = categorical(alpha(x))\n",
    "        if ys is None:\n",
    "            alpha = self.encoder_y(xs)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
    "\n",
    "        # sample (and score) the latent handwritting-style with the variational\n",
    "        # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "        loc, scale = self.encoder_z([xs, ys])\n",
    "        pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "#setup the optimzer\n",
    "adam_params = {\"lr\": 0.0003}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "#setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Summing Out Discrete Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(model, config_enumerate(guide), optimizer, loss = TraceEnum_ELBO(max_plate_nesting=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_classify(self, xs, ys=None):\n",
    "    pyro.module(\"ss_vae\", self)\n",
    "    with pyro.plate(\"data\"):\n",
    "        # this here is the extra term to yield an auxiliary loss\n",
    "        # that we do gradient descent on\n",
    "        if ys is not None:\n",
    "            alpha = self.encoder_y(xs)\n",
    "            with pyro.poutine.scale(scale=self.aux_loss_multiplier):\n",
    "                pyro.sample(\"y_aux\", dist.OneHotCategorical(alpha), obs=ys)\n",
    "\n",
    "def guide_classify(xs, ys):\n",
    "    # the guide is trivial, since there are no\n",
    "    # latent random variables\n",
    "    pass\n",
    "\n",
    "svi_aux = SVI(model_classify, guide_classify, optimizer, loss=Trace_ELBO())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
