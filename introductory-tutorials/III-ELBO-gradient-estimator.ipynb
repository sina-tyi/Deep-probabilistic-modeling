{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A complete example with baselines\n",
    "\n",
    "Recall that in the SVI (stochastic variational inference) tutorial we considered a bernoulli-beta model for coin flips. Because the beta random variable is non-reparameterizable (or rather not easily reparameterizable), the corresponding ELBO gradients can be quite noisy. In that context we dealt with this problem by using a Beta distribution that provides (approximate) reparameterized gradients. Here we showcase how a simple decaying average baseline can reduce the variance in the case where the Beta distribution is treated as non-reparameterized (so that the ELBO gradient estimator is of the score function type). While we’re at it, we also use plate to write our model in a fully vectorized manner.\n",
    "\n",
    "Instead of directly comparing gradient variances, we’re going to see how many steps it takes for SVI to converge. Recall that for this particular model (because of conjugacy) we can compute the exact posterior. So to assess the utility of baselines in this context, we setup the following simple experiment. We initialize the guide at a specified set of variational parameters. We then do SVI until the variational parameters have gotten to within a fixed tolerance of the parameters of the exact posterior. We do this both with and without the decaying average baseline. We then compare the number of gradient steps we needed in the two cases. Here’s the complete code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 189 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7827 & 0.7999\n",
      "Doing inference with use_decaying_avg_baseline=False\n",
      ".\n",
      "Did 75 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7986 & 0.7787\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "# Pyro also has a reparameterized Beta distribution so we import\n",
    "# the non-reparameterized version to make our point\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "import sys\n",
    "\n",
    "assert pyro.__version__.startswith('1.8.4')\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "max_steps = 2 if smoke_test else 10000\n",
    "\n",
    "\n",
    "def param_abs_error(name, target):\n",
    "    return torch.sum(torch.abs(target - pyro.param(name))).item()\n",
    "\n",
    "\n",
    "class BernoulliBetaExample:\n",
    "    def __init__(self, max_steps):\n",
    "        # the maximum number of inference steps we do\n",
    "        self.max_steps = max_steps\n",
    "        # the two hyperparameters for the beta prior\n",
    "        self.alpha0 = 10.0\n",
    "        self.beta0 = 10.0\n",
    "        # the dataset consists of six 1s and four 0s\n",
    "        self.data = torch.zeros(10)\n",
    "        self.data[0:6] = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "        # compute the alpha parameter of the exact beta posterior\n",
    "        self.alpha_n = self.data.sum() + self.alpha0\n",
    "        # compute the beta parameter of the exact beta posterior\n",
    "        self.beta_n = - self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n",
    "        # initial values of the two variational parameters\n",
    "        self.alpha_q_0 = 15.0\n",
    "        self.beta_q_0 = 15.0\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        # sample `latent_fairness` from the beta prior\n",
    "        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n",
    "        # use plate to indicate that the observations are\n",
    "        # conditionally independent given f and get vectorization\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            # observe all ten datapoints using the bernoulli likelihood\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        # register the two variational parameters with pyro\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0),\n",
    "                             constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0),\n",
    "                            constraint=constraints.positive)\n",
    "        # sample f from the beta variational distribution\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline,\n",
    "                         'baseline_beta': 0.90}\n",
    "        # note that the baseline_dict specifies whether we're using\n",
    "        # decaying average baselines or not\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q),\n",
    "                    infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n",
    "        # clear the param store in case we're in a REPL\n",
    "        pyro.clear_param_store()\n",
    "        # setup the optimizer and the inference algorithm\n",
    "        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        # do up to this many steps of inference\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # compute the distance to the parameters of the true posterior\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            # stop inference early if we're close to the true posterior\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"were %.4f & %.4f\") % (alpha_error, beta_error))\n",
    "\n",
    "# do the experiment\n",
    "bbe = BernoulliBetaExample(max_steps=max_steps)\n",
    "bbe.do_inference(use_decaying_avg_baseline=True)\n",
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyroVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
