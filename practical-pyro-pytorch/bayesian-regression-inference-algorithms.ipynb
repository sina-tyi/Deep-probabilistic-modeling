{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression - Inference Algorithms\n",
    "\n",
    "In the previous \"bayesian regression\" example, we looked at how to perform inference on a simple Bayesian linear regression model using SVI. In this tutorial, we’ll explore more expressive guides as well as exact inference techniques. We’ll use the same dataset as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "\n",
    "pyro.set_rng_seed(1)\n",
    "assert pyro.__version__.startswith('1.8.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.set_rng_seed(1)\n",
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "rugged_data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "Our goal is once again to predict log GDP per capita of a nation as a function of two features from the dataset - whether the nation is in Africa, and its Terrain Ruggedness Index, but we will explore more expressive guides.\n",
    "\n",
    "## Model + Guide\n",
    "We will write out the model again, similar to that in \"bayesian regression\", but explicitly without the use of PyroModule. We will write out each term in the regression, using the same priors. bA and bR are regression coefficients corresponding to is_cont_africa and ruggedness, a is the intercept, and bAR is the correlating factor between the two features.\n",
    "\n",
    "Writing down a guide will proceed in close analogy to the construction of our model, with the key difference that the guide parameters need to be trainable. To do this we register the guide parameters in the ParamStore using pyro.param(). Note the positive constraints on scale parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(is_cont_africa, ruggedness, log_gdp):\n",
    "    a = pyro.sample(\"a\", dist.Normal(0., 10.))\n",
    "    b_a = pyro.sample(\"bA\", dist.Normal(0., 1.))\n",
    "    b_r = pyro.sample(\"bR\", dist.Normal(0., 1.))\n",
    "    b_ar = pyro.sample(\"bAR\", dist.Normal(0., 1.))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "    mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness\n",
    "    with pyro.plate(\"data\", len(ruggedness)):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=log_gdp)\n",
    "\n",
    "def guide(is_cont_africa, ruggedness, log_gdp):\n",
    "    a_loc = pyro.param('a_loc', torch.tensor(0.))\n",
    "    a_scale = pyro.param('a_scale', torch.tensor(1.),\n",
    "                         constraint=constraints.positive)\n",
    "    sigma_loc = pyro.param('sigma_loc', torch.tensor(1.),\n",
    "                             constraint=constraints.positive)\n",
    "    weights_loc = pyro.param('weights_loc', torch.randn(3))\n",
    "    weights_scale = pyro.param('weights_scale', torch.ones(3),\n",
    "                               constraint=constraints.positive)\n",
    "    a = pyro.sample(\"a\", dist.Normal(a_loc, a_scale))\n",
    "    b_a = pyro.sample(\"bA\", dist.Normal(weights_loc[0], weights_scale[0]))\n",
    "    b_r = pyro.sample(\"bR\", dist.Normal(weights_loc[1], weights_scale[1]))\n",
    "    b_ar = pyro.sample(\"bAR\", dist.Normal(weights_loc[2], weights_scale[2]))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Normal(sigma_loc, torch.tensor(0.05)))\n",
    "    mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to print latent sites' quantile information.\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        marginal_site = pd.DataFrame(values)\n",
    "        describe = marginal_site.describe(percentiles=[.05, 0.25, 0.5, 0.75, 0.95]).transpose()\n",
    "        site_stats[site_name] = describe[[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n",
    "\n",
    "# Prepare training data\n",
    "df = rugged_data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "train = torch.tensor(df.values, dtype=torch.float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Variational Inference (SVI)\n",
    "As before, we will use SVI to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elbo loss: 5795.468078792095\n",
      "Elbo loss: 415.8169753551483\n",
      "Elbo loss: 250.71914893388748\n",
      "Elbo loss: 247.19455862045288\n",
      "Elbo loss: 249.20037311315536\n",
      "Elbo loss: 250.9648752808571\n",
      "Elbo loss: 249.350925385952\n",
      "Elbo loss: 248.78312557935715\n",
      "Elbo loss: 248.62142199277878\n",
      "Elbo loss: 250.42744493484497\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "\n",
    "svi = SVI(model,\n",
    "          guide,\n",
    "          optim.Adam({\"lr\": .05}),\n",
    "          loss=Trace_ELBO())\n",
    "\n",
    "is_cont_africa, ruggedness, log_gdp = train[:, 0], train[:, 1], train[:, 2]\n",
    "pyro.clear_param_store()\n",
    "num_iters = 5000 if not smoke_test else 2\n",
    "for i in range(num_iters):\n",
    "    elbo = svi.step(is_cont_africa, ruggedness, log_gdp)\n",
    "    if i % 500 == 0:\n",
    "        logging.info(\"Elbo loss: {}\".format(elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "\n",
    "num_samples = 1000\n",
    "predictive = Predictive(model, guide=guide, num_samples=num_samples)\n",
    "svi_samples = {k: v.reshape(num_samples).detach().cpu().numpy()\n",
    "               for k, v in predictive(log_gdp, is_cont_africa, ruggedness).items()\n",
    "               if k != \"obs\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets observe the posterior distribution over the different latent variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: a\n",
      "       mean       std        5%       25%      50%       75%       95%\n",
      "0  9.177024  0.059607  9.078109  9.140462  9.17821  9.217097  9.271518 \n",
      "\n",
      "Site: bA\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0 -1.890622  0.122805 -2.088489 -1.979106 -1.887475 -1.803683 -1.700853 \n",
      "\n",
      "Site: bR\n",
      "       mean       std       5%       25%       50%       75%       95%\n",
      "0 -0.157847  0.039538 -0.22324 -0.183672 -0.157872 -0.133102 -0.091713 \n",
      "\n",
      "Site: bAR\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  0.304515  0.067683  0.194583  0.259464  0.304907  0.348932  0.415127 \n",
      "\n",
      "Site: sigma\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  0.902898  0.047971  0.824166  0.870317  0.901982  0.935171  0.981577 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site, values in summary(svi_samples).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMC\n",
    "In contrast to using variational inference which gives us an approximate posterior over our latent variables, we can also do exact inference using Markov Chain Monte Carlo (MCMC), a class of algorithms that in the limit, allow us to draw unbiased samples from the true posterior. The algorithm that we will be using is called the No-U Turn Sampler (NUTS), which provides an efficient and automated way of running Hamiltonian Monte Carlo. It is slightly slower than variational inference, but provides an exact estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1200/1200 [00:09, 129.22it/s, step size=4.01e-01, acc. prob=0.925]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "nuts_kernel = NUTS(model)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200)\n",
    "mcmc.run(is_cont_africa, ruggedness, log_gdp)\n",
    "\n",
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: a\n",
      "       mean       std       5%       25%       50%       75%       95%\n",
      "0  9.185199  0.135071  8.95266  9.095064  9.182898  9.281514  9.399463 \n",
      "\n",
      "Site: bA\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0 -1.838934  0.223078 -2.208894 -1.982864 -1.833991 -1.687717 -1.482264 \n",
      "\n",
      "Site: bAR\n",
      "       mean       std        5%      25%       50%       75%       95%\n",
      "0  0.340586  0.130707  0.142221  0.24204  0.338872  0.424859  0.562012 \n",
      "\n",
      "Site: bR\n",
      "       mean       std        5%       25%       50%       75%      95%\n",
      "0 -0.183039  0.076419 -0.305314 -0.232908 -0.183521 -0.130119 -0.05983 \n",
      "\n",
      "Site: sigma\n",
      "       mean       std        5%       25%       50%       75%      95%\n",
      "0  0.953522  0.054016  0.871306  0.917909  0.950065  0.986051  1.04744 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site, values in summary(hmc_samples).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyroVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
