{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Great models can only be achieved by iterative development.\n",
    "\n",
    "* Iterate quickly by building a pipeline that is robust to code changes.\n",
    "\n",
    "* Start with a simple model and mean-field inference.\n",
    "\n",
    "* Avoid NANs by intelligently initializing and .clamp()ing.\n",
    "\n",
    "* Reparametrize the model to improve geometry.\n",
    "\n",
    "* Create a custom variational family by combining AutoGuides or EasyGuides.\n",
    "\n",
    "### Table of contents\n",
    "* Overview\n",
    "\n",
    "* Running example: SARS-CoV-2 strain prediction\n",
    "\n",
    "    * Clean the data\n",
    "\n",
    "    * Create a generative model\n",
    "\n",
    "    * Sanity check using mean-field inference\n",
    "\n",
    "    * Create an initialization heuristic\n",
    "\n",
    "    * Reparametrize the model\n",
    "\n",
    "    * Customize the variational family: autoguides, easyguides, custom guides\n",
    "\n",
    "### Overview \n",
    "\n",
    "Consider the problem of sampling from the posterior distribution of a probabilistic model with \n",
    " or more continuous latent variables, but whose data fits entirely in memory. (For larger datasets, consider amortized variational inference.) Inference in such high-dimensional models can be challenging even when posteriors are known to be unimodal or even log-concave, due to correlations among latent variables.\n",
    "\n",
    "To perform inference in such high-dimensional models in Pyro, we have evolved a workflow to incrementally build data analysis pipelines combining variational inference, reparametrization effects, and ad-hoc initialization strategies. Our workflow is summarized as a sequence of steps, where validation after any step might suggest backtracking to change design decisions at a previous step.\n",
    "\n",
    "The crux of efficient workflow is to ensure changes don’t break your pipeline. That is, after you build a number of pipeline stages, validate results, and decide to change one component in the pipeline, you’d like to minimize code changes needed in other components. The remainder of this tutorial describes these steps individually, then describes nuances of interactions among stages, then provides an example.\n",
    "\n",
    "### Running example: SARS-CoV-2 strain prediction\n",
    "\n",
    "The running example in this tutorial will be a model of the relative growth rates of different strains of the SARS-CoV-2 virus, based on open data counting different PANGO lineages of viral genomic samples collected at different times around the world. There are about 2 million sequences in total.\n",
    "\n",
    "The model is a high-dimensional regression model with around 1000 coefficients, a multivariate logistic growth function (using a simple torch.softmax()) and a Multinomial likelihood. While the number of coefficients is relatively small, there are about 500,000 local latent variables to estimate, and plate structure in the model should lead to an approximately block diagonal posterior covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sina/Library/Mobile Documents/com~apple~CloudDocs/Git_Projects/Deep-probabilistic-modeling/PyroVenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uisng CPU\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions import constraints\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import (\n",
    "    AutoDelta,\n",
    "    AutoNormal,\n",
    "    AutoMultivariateNormal,\n",
    "    AutoGuideList,\n",
    "    init_to_feasible,\n",
    ")\n",
    "\n",
    "from pyro.infer.reparam import AutoReparam, LocScaleReparam\n",
    "from pyro.nn.module import PyroParam\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyro.ops.special import sparse_multinomial_likelihood\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    print(\"Uisng CPU\")\n",
    "smoke_test = ('CI' in os.environ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "Our running example will use a pre-cleaned dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: Tensor of shape (27, 202, 1316) on cpu\n",
      "features: Tensor of shape (1316, 2634) on cpu\n",
      "lineages: list of length 1316\n",
      "locations: list of length 202\n",
      "mutations: list of length 2634\n",
      "sparse_counts.index: Tensor of shape (3, 57129) on cpu\n",
      "sparse_counts.total: Tensor of shape (27, 202) on cpu\n",
      "sparse_counts.value: Tensor of shape (57129,) on cpu\n",
      "start_date: datetime\n",
      "time_step_days: int\n"
     ]
    }
   ],
   "source": [
    "from pyro.contrib.examples.nextstrain import load_nextstrain_counts\n",
    "dataset = load_nextstrain_counts()\n",
    "\n",
    "def summarize(x, name=\"\"):\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in sorted(x.items()):\n",
    "            summarize(v, name + \".\" + k if name else k)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        print(f\"{name}: {type(x).__name__} of shape {tuple(x.shape)} on {x.device}\")\n",
    "    elif isinstance(x, list):\n",
    "        print(f\"{name}: {type(x).__name__} of length {len(x)}\")\n",
    "    else:\n",
    "        print(f\"{name}: {type(x).__name__}\")\n",
    "summarize(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a generative model\n",
    "\n",
    "The first step to using Pyro is creating a generative model, either a python function or a pyro.nn.Module. Start simple. Start with a shallow hierarchy and later add latent variables to share statistical strength. Start with a slice of your data then add a plate over multiple slices. Start with simple distributions like Normal, LogNormal, Poisson and Multinomial, then consider overdispersed versions like StudentT, Gamma, GammaPoisson/NegativeBinomial, and DirichletMultinomial. Keep your model simple and readable so you can share it and get feedback from domain experts. Use weakly informative priors.\n",
    "\n",
    "We’ll focus on a multivariate logistic growth model of competing SARS-CoV-2 strains, as described in Obermeyer et al. (2022). This model uses a numerically stable logits parameter in its multinomial likelihood, rather than a probs parameter. Similarly upstream variables init, rate, rate_loc, and coef are all in log-space. This will mean e.g. that a zero coefficient has multiplicative effect of 1.0, and a positive coefficient has multiplicative effect greater than 1.\n",
    "\n",
    "Note we scale coef by 1/100 because we want to model a very small number, but the automatic parts of Pyro and PyTorch work best for numbers on the order of 1.0 rather than very small numbers. When we later interpret coef in a volcano plot we’ll need to duplicate this scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dataset):\n",
    "    features = dataset[\"features\"]\n",
    "    counts = dataset[\"counts\"]\n",
    "    assert features.shape[0] == counts.shape[-1]\n",
    "    S, M = features.shape\n",
    "    T, P, S = counts.shape\n",
    "    time = torch.arande(float(T)) * dataset[\"time_step_days\"] / 5.5\n",
    "    time -= time.mean()\n",
    "    strain_plate = pyro.plate(\"strain\", S, dim=-1)\n",
    "    place_plate = pyro.plate(\"place\", P, dim=-2)\n",
    "    time_plate = pyro.plate(\"time\", T, dim=-3)\n",
    "\n",
    "    # Model each region as multivariate logistic growth\n",
    "    rate_scale = pyro.sample(\"rate_scale\", dist.LogNormal(-4, 2))\n",
    "    init_scale = pyro.sample(\"init_scale\", dist.LogNormal(0, 2))\n",
    "    with pyro.plate(\"mutation\", M, dim=-1):\n",
    "        coef = pyro.sample(\"coef\", dist.Laplace(0, 0.5))\n",
    "    with strain_plate:\n",
    "        rate_loc = pyro.deterministic(\"rate_loc\", 0.01 * coef @ features.T)\n",
    "    with place_plate, strain_plate:\n",
    "        rate = pyro.sample(\"rate\", dist.Normal(rate_loc, rate_scale))\n",
    "        init = pyro.sample(\"init\", dist.Normal(0, init_scale))\n",
    "    logits = init + rate * time[:, None, None]\n",
    "\n",
    "    # observe sequences via a multinomial likelihood\n",
    "    with time_plate, place_plate:\n",
    "        pyro.sample(\n",
    "            \"obs\",\n",
    "            dist.Multinomial(logits=logits.unsqueeze(-2), validate_args=False),\n",
    "            obs=counts.unsqueeze(-2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts has 57129 / 7177464 nonzero elements\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The execution cost of this model is dominated by the multinomial likelihood over a large sparse count matrix.\n",
    "'''\n",
    "\n",
    "print(\"counts has {:d} / {} nonzero elements\".format(\n",
    "    dataset['counts'].count_nonzero(), dataset['counts'].numel()\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up inference (and model iteration!) we’ll replace the pyro.sample(..., Multinomial) likelihood with an equivalent but much cheaper pyro.factor statement using a helper pyro.ops.sparse_multinomial_likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dataset, predict=None):\n",
    "    features = dataset[\"features\"]\n",
    "    counts = dataset[\"counts\"]\n",
    "    sparse_counts = dataset[\"sparse_counts\"]\n",
    "    assert features.shape[0] == counts.shape[-1]\n",
    "    S, M = features.shape\n",
    "    T, P, S = counts.shape\n",
    "    time = torch.arange(float(T) * dataset[\"TIME_STEP_DAYS\"]) / 5.5\n",
    "    time -= time.mean()\n",
    "\n",
    "    # Model each region as multivariate logistic growth\n",
    "    rate_scale = pyro.sample(\"rate_scale\", dist.LogNormal(-4, 2))\n",
    "    init_scale = pyro.sample(\"init_scale\", dist.LogNormal(0, 2))\n",
    "    with pyro.plate(\"mutation\", M, dim=-1):\n",
    "        coef = pyro.sample(\"coef\", dist.Laplace(0, 0.5))\n",
    "    with pyro.plate(\"strain\", S, dim=-1):\n",
    "        rate_loc = pyro.deterministic(\"rate_loc\", 0.01 * coef @ features.T)\n",
    "        with pyro.plate(\"place\", P, dim=-2):\n",
    "            rate = pyro.sample(\"rate\", dist.Normal(rate_loc, rate_scale))\n",
    "            init = pyro.sample(\"init\", dist.Normal(0, init_scale))\n",
    "    if predict is not None:   # exit early during evaluation\n",
    "        probs = (init + rate * time[predict]).softmax(-1)\n",
    "        return probs\n",
    "    logits = (init + rate * time[:, None, None]).log_sotmax(-1)\n",
    "\n",
    "    # observe sequences via a cheap sparse multinomial likelihood\n",
    "    t, p, s = sparse_counts[\"index\"]\n",
    "    pyro.factor(\n",
    "        \"obs\",\n",
    "        sparse_multinomial_likelihood(\n",
    "            sparse_counts[\"total\"], logits[t, p, s], sparse_counts[\"value\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check using mean field inference\n",
    "Mean field Normal inference is cheap and robust, and is a good way to sanity check your posterior point estimate, even if the posterior uncertainty may be implausibly narrow. It is recommended starting with an AutoNormal guide, and possibly setting init_scale to a small value like init_scale=0.01 or init_scale=0.001.\n",
    "\n",
    "Note that while MAP estimating via AutoDelta is even cheaper and more robust than mean-field AutoNormal, AutoDelta is coordinate-system dependent and is not invariant to reparametrization. Because in our experience most models benefit from some reparameterization, we recommend AutoNormal over AutoDelta because AutoNormal is less sensitive to reparametrization (AutoDelta can give incorrect results in some reparametrized models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svi(model, guide,lr=0.01, num_steps=1001, log_every=100, plot=True):\n",
    "    pyro.clear_param_store()\n",
    "    pyro.set_rng_seed(20211205)\n",
    "    if smoke_test:\n",
    "        num_steps = 2\n",
    "\n",
    "    # Measure model and guide complexity.\n",
    "    num_latents = sum(\n",
    "        site[\"value\"].numel()\n",
    "        for name, site in poutine.trace(guide).get_trace(dataset).iter_stochastic_nodes()\n",
    "        if not site[\"infer\"].get(\"is_auxiliary\")\n",
    "    )\n",
    "    num_params = sum(p.unconstrained().numel() for p in pyro.get_param_store().values())\n",
    "    print(f\"Found {num_latents} latent variables and {num_params} learnable parameters\")\n",
    "\n",
    "    # Save graident norms during inference\n",
    "    series = defaultdict(list)\n",
    "    def hook(g, series):\n",
    "        series.append(torch.linalg.norm(g.reshape(-1), math.inf).item())\n",
    "    for name, value in pyro.get_parm_store().named_parameters():\n",
    "        value.register_hook(\n",
    "            functools.partial(hook, series = series[name + \"grad\"])\n",
    "        )\n",
    "\n",
    "    # Train the guide\n",
    "    optim = ClippedAdam({\"lr\": lr, \"lrd\": 0.1 ** (1 / num_steps)})\n",
    "    svi = SVI(model, guide, optim, Trace_ELBO())\n",
    "    num_obs = int(dataset[\"counts\"].count_nonzero())\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(dataset / num_obs)\n",
    "        series[\"loss\"].append(loss)\n",
    "        median = guide.median()  # cheap for autoguides\n",
    "        for name, value in median.items():\n",
    "            if value.numel() == 1:\n",
    "                series[name + \"mean\"].append(float(value))\n",
    "            if step % log_every == 0:\n",
    "                print(f\"step {step: >4d} loss = {loss:0.6g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyroVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
